<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Analysing CSV file using ELK | MrJcK</title><meta name=keywords content><meta name=description content="There are many situations when we want to analyze large size of logs. Considering the logs are in CSV format. There are many tools that make this task easy. Still, it’s frustrating us with the amount of time it takes to process the data.
To overcome this I took the help of ELK (Docker) in a windows machine using WSL.
Requirement Docker in Windows WSL 2 (Ubuntu/Kali) Installation ELK docker installation"><meta name=author content><link rel=canonical href=https://mrjck.github.io/posts/analysing-csv-file-using-elk/><link crossorigin=anonymous href=/assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe+FVUFzPh7U=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://mrjck.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://mrjck.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://mrjck.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://mrjck.github.io/apple-touch-icon.png><link rel=mask-icon href=https://mrjck.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Analysing CSV file using ELK"><meta property="og:description" content="There are many situations when we want to analyze large size of logs. Considering the logs are in CSV format. There are many tools that make this task easy. Still, it’s frustrating us with the amount of time it takes to process the data.
To overcome this I took the help of ELK (Docker) in a windows machine using WSL.
Requirement Docker in Windows WSL 2 (Ubuntu/Kali) Installation ELK docker installation"><meta property="og:type" content="article"><meta property="og:url" content="https://mrjck.github.io/posts/analysing-csv-file-using-elk/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-15T13:05:49+03:00"><meta property="article:modified_time" content="2023-07-15T13:05:49+03:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Analysing CSV file using ELK"><meta name=twitter:description content="There are many situations when we want to analyze large size of logs. Considering the logs are in CSV format. There are many tools that make this task easy. Still, it’s frustrating us with the amount of time it takes to process the data.
To overcome this I took the help of ELK (Docker) in a windows machine using WSL.
Requirement Docker in Windows WSL 2 (Ubuntu/Kali) Installation ELK docker installation"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://mrjck.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Analysing CSV file using ELK","item":"https://mrjck.github.io/posts/analysing-csv-file-using-elk/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Analysing CSV file using ELK","name":"Analysing CSV file using ELK","description":"There are many situations when we want to analyze large size of logs. Considering the logs are in CSV format. There are many tools that make this task easy. Still, it’s frustrating us with the amount of time it takes to process the data.\nTo overcome this I took the help of ELK (Docker) in a windows machine using WSL.\nRequirement Docker in Windows WSL 2 (Ubuntu/Kali) Installation ELK docker installation","keywords":[],"articleBody":"There are many situations when we want to analyze large size of logs. Considering the logs are in CSV format. There are many tools that make this task easy. Still, it’s frustrating us with the amount of time it takes to process the data.\nTo overcome this I took the help of ELK (Docker) in a windows machine using WSL.\nRequirement Docker in Windows WSL 2 (Ubuntu/Kali) Installation ELK docker installation\nsudo docker pull sebp/elk Starting docker\ndocker run -p 5601:5601 -p 9200:9200 -p 5044:5044 -it -e LOGSTASH_START=0 -v {CSV file location in host}:{Mounting location} --name Container_Name sebp/elk e LOGSTASH_START=0: Start ELK without logstash v {CSV file location in host}:{Mounting location}:Mount host drive to docker Starting Docker with bash terminal\nOpen new terminal and enter below command:\ndocker exec -it Container_name /bin/bash Then upload CSV Logs using command below:\n/opt/logstash/bin/logstash -f logstash.conf logstash.conf Example\ninput { file { path =\u003e \"/home/user/log.csv\" start_position =\u003e \"beginning\" # sincedb_path =\u003e \"/dev/null\" } } filter { csv { separator =\u003e \",\" columns =\u003e [\"Sr\",\"Day\",\"Weekday\",\"District\",\"DistrictNumber\",\"Mem\",\"Abs\",\"AttendancePercent\",\"AbsentPercent\",\"County\" ] } } output { elasticsearch { hosts =\u003e \"http://localhost:9200\" index =\u003e \"elk_test\" } stdout {} } When we add logs using logstash timeline created will the time when log has uploaded to elastic search.\nTo mitigate this we need to create gork filter to remove the timestamp filed created by logstash and map time column we want to map as below:\nfile{ path =\u003e \"C:/log_files/*.csv\" sincedb_path =\u003e \"NULL\" start_position =\u003e \"beginning\" } } filter{ csv{ separator =\u003e \",\" columns =\u003e [\"Event Time\",\"Receipt Time\",\"Device\",\"startTime\"] remove_field =\u003e [\"@timestamp\"] } date { match =\u003e [\"startTime\",\"yyyy/MM/dd HH:mm:ss ZZZ\"] target =\u003e \"startTime\" } } output{ elasticsearch{ hosts =\u003e [\"localhost:9200\"] index =\u003e \"fortigate_201_07_1022\" user =\u003e \"elastic\" password =\u003e \"e3yC******\" } stdout{ codec =\u003e rubydebug } } ","wordCount":"291","inLanguage":"en","datePublished":"2023-07-15T13:05:49+03:00","dateModified":"2023-07-15T13:05:49+03:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mrjck.github.io/posts/analysing-csv-file-using-elk/"},"publisher":{"@type":"Organization","name":"MrJcK","logo":{"@type":"ImageObject","url":"https://mrjck.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://mrjck.github.io accesskey=h title="MrJcK (Alt + H)">MrJcK</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mrjck.github.io/myconfig title=MyConfig><span>MyConfig</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://mrjck.github.io>Home</a>&nbsp;»&nbsp;<a href=https://mrjck.github.io/posts/>Posts</a></div><h1 class=post-title>Analysing CSV file using ELK</h1><div class=post-meta><span title='2023-07-15 13:05:49 +0300 +03'>July 15, 2023</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;291 words</div></header><div class=post-content><p>There are many situations when we want to analyze large size of logs. Considering the logs are in CSV format. There are many tools that make this task easy. Still, it’s frustrating us with the amount of time it takes to process the data.</p><p>To overcome this I took the help of ELK (Docker) in a windows machine using WSL.</p><h3 id=requirement><strong>Requirement</strong><a hidden class=anchor aria-hidden=true href=#requirement>#</a></h3><ul><li>Docker in Windows</li><li>WSL 2 (Ubuntu/Kali)</li></ul><h3 id=installation><strong>Installation</strong><a hidden class=anchor aria-hidden=true href=#installation>#</a></h3><p><strong>ELK docker installation</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo docker pull sebp/elk
</span></span></code></pre></div><p><strong>Starting docker</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker run -p 5601:5601 -p 9200:9200 -p 5044:5044 -it -e LOGSTASH_START<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span> -v <span style=color:#f92672>{</span>CSV file location in host<span style=color:#f92672>}</span>:<span style=color:#f92672>{</span>Mounting location<span style=color:#f92672>}</span> --name Container_Name sebp/elk
</span></span></code></pre></div><ul><li><code>e LOGSTASH_START=0:</code> Start ELK without logstash</li><li><code>v {CSV file location in host}:{Mounting location}:</code>Mount host drive to docker</li></ul><p><strong>Starting Docker with bash terminal</strong></p><p>Open new terminal and enter below command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker exec -it Container_name /bin/bash
</span></span></code></pre></div><p>Then upload CSV Logs using command below:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>/opt/logstash/bin/logstash -f logstash.conf
</span></span></code></pre></div><hr><p>logstash.conf Example</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-toml data-lang=toml><span style=display:flex><span><span style=color:#a6e22e>input</span> {
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>file</span> {
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>path</span> =<span style=color:#960050;background-color:#1e0010>&gt;</span> <span style=color:#e6db74>&#34;/home/user/log.csv&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>start_position</span> =<span style=color:#960050;background-color:#1e0010>&gt;</span> <span style=color:#e6db74>&#34;beginning&#34;</span>
</span></span><span style=display:flex><span>       <span style=color:#75715e># sincedb_path =&gt; &#34;/dev/null&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>filter</span> {
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>csv</span> {
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>separator</span> =<span style=color:#960050;background-color:#1e0010>&gt;</span> <span style=color:#e6db74>&#34;,&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>columns</span> =<span style=color:#960050;background-color:#1e0010>&gt;</span>
</span></span><span style=display:flex><span>        [<span style=color:#e6db74>&#34;Sr&#34;</span>,<span style=color:#e6db74>&#34;Day&#34;</span>,<span style=color:#e6db74>&#34;Weekday&#34;</span>,<span style=color:#e6db74>&#34;District&#34;</span>,<span style=color:#e6db74>&#34;DistrictNumber&#34;</span>,<span style=color:#e6db74>&#34;Mem&#34;</span>,<span style=color:#e6db74>&#34;Abs&#34;</span>,<span style=color:#e6db74>&#34;AttendancePercent&#34;</span>,<span style=color:#e6db74>&#34;AbsentPercent&#34;</span>,<span style=color:#e6db74>&#34;County&#34;</span>
</span></span><span style=display:flex><span>        ]
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>output</span> {
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>elasticsearch</span> {
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>hosts</span> =<span style=color:#960050;background-color:#1e0010>&gt;</span> <span style=color:#e6db74>&#34;http://localhost:9200&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>index</span> =<span style=color:#960050;background-color:#1e0010>&gt;</span> <span style=color:#e6db74>&#34;elk_test&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>stdout</span> {}
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>When we add logs using logstash timeline created will the time when log has uploaded to elastic search.</p><p>To mitigate this we need to create gork filter to remove the timestamp filed created by logstash and map time column we want to map as below:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-toml data-lang=toml><span style=display:flex><span><span style=color:#a6e22e>file</span>{
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>path</span> =<span style=color:#960050;background-color:#1e0010>&gt;</span> <span style=color:#e6db74>&#34;C:/log_files/*.csv&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>sincedb_path</span> =<span style=color:#960050;background-color:#1e0010>&gt;</span> <span style=color:#e6db74>&#34;NULL&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>start_position</span> =<span style=color:#960050;background-color:#1e0010>&gt;</span> <span style=color:#e6db74>&#34;beginning&#34;</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span><span style=color:#a6e22e>filter</span>{
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>csv</span>{
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>separator</span> =<span style=color:#960050;background-color:#1e0010>&gt;</span> <span style=color:#e6db74>&#34;,&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>columns</span> =<span style=color:#960050;background-color:#1e0010>&gt;</span> [<span style=color:#e6db74>&#34;Event Time&#34;</span>,<span style=color:#e6db74>&#34;Receipt Time&#34;</span>,<span style=color:#e6db74>&#34;Device&#34;</span>,<span style=color:#e6db74>&#34;startTime&#34;</span>]
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>remove_field</span> =<span style=color:#960050;background-color:#1e0010>&gt;</span> [<span style=color:#e6db74>&#34;@timestamp&#34;</span>]
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>date</span> {
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>match</span> =<span style=color:#960050;background-color:#1e0010>&gt;</span> [<span style=color:#e6db74>&#34;startTime&#34;</span>,<span style=color:#e6db74>&#34;yyyy/MM/dd HH:mm:ss ZZZ&#34;</span>]
</span></span><span style=display:flex><span>            <span style=color:#a6e22e>target</span> =<span style=color:#960050;background-color:#1e0010>&gt;</span> <span style=color:#e6db74>&#34;startTime&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span><span style=color:#a6e22e>output</span>{
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>elasticsearch</span>{
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>hosts</span> =<span style=color:#960050;background-color:#1e0010>&gt;</span> [<span style=color:#e6db74>&#34;localhost:9200&#34;</span>]
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>index</span> =<span style=color:#960050;background-color:#1e0010>&gt;</span> <span style=color:#e6db74>&#34;fortigate_201_07_1022&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>user</span> =<span style=color:#960050;background-color:#1e0010>&gt;</span> <span style=color:#e6db74>&#34;elastic&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>password</span> =<span style=color:#960050;background-color:#1e0010>&gt;</span> <span style=color:#e6db74>&#34;e3yC******&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>stdout</span>{
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>codec</span> =<span style=color:#960050;background-color:#1e0010>&gt;</span> <span style=color:#a6e22e>rubydebug</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://mrjck.github.io/posts/log2timeline-using-wsl/><span class=title>« Prev</span><br><span>Log2timeline Using WSL</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://mrjck.github.io>MrJcK</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>